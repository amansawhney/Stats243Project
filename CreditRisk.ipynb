{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n"
     ]
    }
   ],
   "source": [
    "#reading in and splitting data\n",
    "\n",
    "Df = pd.read_csv(\"cs-training.csv\", index_col = 0)\n",
    "print(len(Df))\n",
    "Df = pd.DataFrame(Df)\n",
    "Df = Df.dropna()\n",
    "X_Cols = list(Df.columns)\n",
    "X_Cols.remove(\"SeriousDlqin2yrs\")\n",
    "X = Df[X_Cols].to_numpy()\n",
    "Y = Df[\"SeriousDlqin2yrs\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resampling to correct data imbalance\n",
    "\n",
    "Default = []\n",
    "for i in range(len(X_Train)):\n",
    "    if(Y_Train[i] == 1):\n",
    "        Default.append(X_Train[i])\n",
    "Default = np.asarray(Default)\n",
    "        \n",
    "Resampled = []\n",
    "Resampled_y = []\n",
    "Target = X_Train.shape[0]/2\n",
    "Current = np.sum(Y_Train)\n",
    "\n",
    "while(Current <= Target):\n",
    "    Index = np.random.choice(Default.shape[0], 1)  \n",
    "    Resampled.append(Default[Index].squeeze())\n",
    "    Resampled_y.append(1)\n",
    "    Current += 1\n",
    "    Target += .498\n",
    "    \n",
    "X_Train_resamp = np.concatenate((X_Train, np.asarray(Resampled)))\n",
    "Y_Train_resamp = np.concatenate((Y_Train, np.asarray(Resampled_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Premilinaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "plt.figure(figsize = (10, 10))\n",
    "Corr_Matrix = Df.corr()\n",
    "round(Corr_Matrix, 2)\n",
    "Fig = sns.heatmap(Corr_Matrix, annot = True)\n",
    "Figure = Fig.get_figure()    \n",
    "Figure.savefig('CorrPlot.pdf', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Two by Two Plots\n",
    "Plt = sns.pairplot(Df)\n",
    "Plt\n",
    "Figure1 = Plt.get_figure()    \n",
    "Figure1.savefig('PairwisePlots.pdf', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisher LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for Train Are:\n",
      "85.44373421738862 83.43138114569095\n",
      "Scores for Test Are:\n",
      "85.49241455142446 83.98570707599899\n"
     ]
    }
   ],
   "source": [
    "mu = np.mean(X_Train)\n",
    "X_Train_Demeaned = (X_Train - mu).T\n",
    "X_Test_Demeaned = (X_Test - mu).T\n",
    "S_t = np.cov(X_Train_Demeaned)\n",
    "S_w = np.zeros(S_t.shape)\n",
    "for c in np.unique(Y_Train):\n",
    "    S_w += np.cov(X_Train_Demeaned[:, Y_Train == c])\n",
    "\n",
    "S_b = S_t - S_w\n",
    "\n",
    "Vals, Vecs = linalg.eig(np.linalg.inv(S_w)@S_b)\n",
    "Vecs = Vecs[:, np.argsort(Vals)]\n",
    "\n",
    "W_lda = Vecs[:, -1:].real\n",
    "\n",
    "X_Train_Lda = (W_lda.T@X_Train_Demeaned).T\n",
    "X_Test_Lda = (W_lda.T@X_Test_Demeaned).T\n",
    "\n",
    "print(\"Scores for Train Are:\")\n",
    "print(np.mean(X_Train_Lda[Y_Train == 0]), np.mean(X_Train_Lda[Y_Train == 1]))\n",
    "\n",
    "print(\"Scores for Test Are:\")\n",
    "print(np.mean(X_Test_Lda[Y_Test == 0]), np.mean(X_Test_Lda[Y_Test == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores Are:\n",
      "inf\n",
      "inf inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9w/fljc9lns4g94k33j57yk2w8r0000gn/T/ipykernel_28415/3928317930.py:26: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  Score = Probs[:, 1]/(1 - Probs[:, 1])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "Clf = LogisticRegression(random_state = 0, max_iter = 1000).fit(X_Train_resamp, Y_Train_resamp)\n",
    "Probs = Clf.predict_proba(X_Train_resamp)\n",
    "\n",
    "Train_Predicted = np.zeros(len(Probs))\n",
    "for i in range(len(Probs)):\n",
    "    if (Probs[i, 0] > Probs[i, 1]):  \n",
    "        Train_Predicted[i] = 0\n",
    "    else:\n",
    "        Train_Predicted[i] = 1  \n",
    "        \n",
    "Equal = 0\n",
    "default = 0\n",
    "non_default = 0\n",
    "for i in range(len(Y_Train_resamp)):\n",
    "    if(Y_Train_resamp[i] == 1):\n",
    "        if(Y_Train_resamp[i] == Train_Predicted[i]):\n",
    "            Equal += 1\n",
    "            default += 1\n",
    "    else:\n",
    "        if(Y_Train_resamp[i] == Train_Predicted[i]):\n",
    "            Equal += 1\n",
    "            non_default += 1        \n",
    "\n",
    "\n",
    "Score = Probs[:, 1]/(1 - Probs[:, 1])\n",
    "print(\"Scores Are:\")\n",
    "print(np.sum(Score[Y_Train_resamp == 0]))\n",
    "print(np.mean(Score[Y_Train_resamp == 0]), np.mean(Score[Y_Train_resamp == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy on Training Set is:\n",
      "0.7261610589445283\n",
      "Classification Accuracy on Non-Default Training Set is:\n",
      "0.8706886207586161\n",
      "Classification Accuracy on Default Training Set is:\n",
      "0.5810997069410805\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Accuracy on Training Set is:\")\n",
    "print(Equal/len(Y_Train_resamp))\n",
    "print(\"Classification Accuracy on Non-Default Training Set is:\")\n",
    "print(non_default/(len(Y_Train_resamp) - np.sum(Y_Train_resamp)))\n",
    "print(\"Classification Accuracy on Default Training Set is:\")\n",
    "print(default/np.sum(Y_Train_resamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores Are:\n",
      "inf inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9w/fljc9lns4g94k33j57yk2w8r0000gn/T/ipykernel_28415/3709622264.py:26: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  Score = Probs[:, 1]/(1 - Probs[:, 1])\n"
     ]
    }
   ],
   "source": [
    "Probs = Clf.predict_proba(X_Test)\n",
    "Test_Predicted = np.zeros(len(Probs))\n",
    "\n",
    "for i in range(len(Probs)):\n",
    "    if (Probs[i, 0] > Probs[i, 1]):  \n",
    "        Test_Predicted[i] = 0\n",
    "    else:\n",
    "        Test_Predicted[i] = 1\n",
    "\n",
    "        \n",
    "Equal_test = 0\n",
    "default_test = 0\n",
    "non_default_test = 0\n",
    "for i in range(len(Y_Test)):\n",
    "    if(Y_Test[i] == 1):\n",
    "        if(Y_Test[i] == Test_Predicted[i]):\n",
    "            Equal_test += 1\n",
    "            default_test += 1\n",
    "    else:\n",
    "        if(Y_Test[i] == Test_Predicted[i]):\n",
    "            Equal_test += 1\n",
    "            non_default_test += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "Score = Probs[:, 1]/(1 - Probs[:, 1])\n",
    "print(\"Scores Are:\")\n",
    "print(np.mean(Score[Y_Test == 0]), np.mean(Score[Y_Test == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy on Test Set is:\n",
      "0.8507143037113558\n",
      "Classification Accuracy on Non-Default Test Set is:\n",
      "0.8708104153683583\n",
      "Classification Accuracy on Default Test Set is:\n",
      "0.5841121495327103\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Accuracy on Test Set is:\")\n",
    "print(Equal_test/len(Y_Test))\n",
    "print(\"Classification Accuracy on Non-Default Test Set is:\")\n",
    "print(non_default_test/(len(Y_Test) - np.sum(Y_Test)))\n",
    "print(\"Classification Accuracy on Default Test Set is:\")\n",
    "print(default_test/np.sum(Y_Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparison with logit for our own checks\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "Logit_Model = sm.Logit(Y_Train, sm.add_constant(X_Train)).fit()\n",
    "print (Logit_Model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Probs = Clf.predict_proba(X_Test)\n",
    "Test_Predicted = np.zeros(len(Probs))\n",
    "\n",
    "for i in range(len(Probs)):\n",
    "    if (Probs[i, 0] > Probs[i, 1]):  \n",
    "        Test_Predicted[i] = 0\n",
    "    else:\n",
    "        Test_Predicted[i] = 1\n",
    "\n",
    "\n",
    "Equal = 0\n",
    "for i in range(len(Y_Test)):\n",
    "    if (Y_Test[i] == Test_Predicted[i]):\n",
    "        Equal = Equal + 1\n",
    "        \n",
    "Score = Probs[:, 1]/(1 - Probs[:, 1])\n",
    "print(\"Mean Estimated Odds Ratio for Test Are:\")\n",
    "print(np.mean(Score[Y_Test == 0]), np.mean(Score[Y_Test == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Accuracy on Test Set is:\")\n",
    "Equal/len(Y_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores Are:\n",
      "0.8313504321271528 4.443956780907908\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "Clf = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "Clf.fit(X_Train_resamp, Y_Train_resamp)\n",
    "Probs = Clf.predict_proba(X_Train_resamp)\n",
    "\n",
    "Train_Predicted = np.zeros(len(Probs))\n",
    "for i in range(len(Probs)):\n",
    "    if (Probs[i, 0] > Probs[i, 1]):  \n",
    "        Train_Predicted[i] = 0\n",
    "    else:\n",
    "        Train_Predicted[i] = 1\n",
    "        \n",
    "Equal = 0\n",
    "default = 0\n",
    "non_default = 0\n",
    "for i in range(len(Y_Train_resamp)):\n",
    "    if(Y_Train_resamp[i] == 1):\n",
    "        if(Y_Train_resamp[i] == Train_Predicted[i]):\n",
    "            Equal += 1\n",
    "            default += 1\n",
    "    else:\n",
    "        if(Y_Train_resamp[i] == Train_Predicted[i]):\n",
    "            Equal += 1\n",
    "            non_default += 1          \n",
    "        \n",
    "        \n",
    "\n",
    "Score = Probs[:, 1]/(1 - Probs[:, 1])\n",
    "print(\"Scores Are:\")\n",
    "print(np.mean(Score[Y_Train_resamp == 0]), np.mean(Score[Y_Train_resamp == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy on Training Set is:\n",
      "0.7751011794248467\n",
      "Classification Accuracy on Non-Default Training Set is:\n",
      "0.7764815678954736\n",
      "Classification Accuracy on Default Training Set is:\n",
      "0.7737156927029667\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Accuracy on Training Set is:\")\n",
    "print(Equal/len(Y_Train_resamp))\n",
    "print(\"Classification Accuracy on Non-Default Training Set is:\")\n",
    "print(non_default/(len(Y_Train_resamp) - np.sum(Y_Train_resamp)))\n",
    "print(\"Classification Accuracy on Default Training Set is:\")\n",
    "print(default/np.sum(Y_Train_resamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Probs = Clf.predict_proba(X_Test)\n",
    "\n",
    "Test_Predicted = np.zeros(len(Probs))\n",
    "\n",
    "for i in range(len(Probs)):\n",
    "    if (Probs[i, 0] > Probs[i, 1]):  \n",
    "        Test_Predicted[i] = 0\n",
    "    else:\n",
    "        Test_Predicted[i] = 1\n",
    "\n",
    "Equal_test = 0\n",
    "default_test = 0\n",
    "non_default_test = 0\n",
    "for i in range(len(Y_Test)):\n",
    "    if(Y_Test[i] == 1):\n",
    "        if(Y_Test[i] == Test_Predicted[i]):\n",
    "            Equal_test += 1\n",
    "            default_test += 1\n",
    "    else:\n",
    "        if(Y_Test[i] == Test_Predicted[i]):\n",
    "            Equal_test += 1\n",
    "            non_default_test += 1\n",
    "\n",
    "Score = Probs[:, 1]/(1 - Probs[:, 1])\n",
    "print(\"Scores Are:\")\n",
    "print(np.mean(Score[Y_Test == 0]), np.mean(Score[Y_Test == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Accuracy on Test Set is:\")\n",
    "print(Equal_test/len(Y_Test))\n",
    "print(\"Classification Accuracy on Non-Default Test Set is:\")\n",
    "print(non_default_test/(len(Y_Test) - np.sum(Y_Test)))\n",
    "print(\"Classification Accuracy on Default Test Set is:\")\n",
    "print(default_test/np.sum(Y_Test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Class_Weight = {1: 0.95,0: 0.05}\n",
    "import random\n",
    "random.seed(1)\n",
    "Model_in = keras.Input(shape = (10, ))\n",
    "X = layers.Dense(10, activation = \"relu\")(Model_in)\n",
    "X2 = layers.Dense(10, activation= \"relu\")(X)\n",
    "X3 = layers.Dense(10, activation= \"relu\")(X2)\n",
    "X4 = layers.Dense(10, activation= \"relu\")(X3)\n",
    "\n",
    "Out = layers.Dense(1, activation= \"sigmoid\")(X4)\n",
    "\n",
    "Model = keras.Model(Model_in, Out)\n",
    "Model.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n",
    "Model.fit(X_Train_resamp, Y_Train_resamp, epochs = 100,\n",
    "                batch_size = 128,\n",
    "                shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Probs = Model.predict(X_Train_resamp)\n",
    "Probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Predicted = np.zeros(len(Probs))\n",
    "for i in range(len(Probs)):\n",
    "    if (Probs[i] < .5):  \n",
    "        Train_Predicted[i] = 0\n",
    "    else:\n",
    "        Train_Predicted[i] = 1\n",
    "\n",
    "Equal = 0\n",
    "default = 0\n",
    "non_default = 0\n",
    "for i in range(len(Y_Train_resamp)):\n",
    "    if(Y_Train_resamp[i] == 1):\n",
    "        if(Y_Train_resamp[i] == Train_Predicted[i]):\n",
    "            Equal += 1\n",
    "            default += 1\n",
    "    else:\n",
    "        if(Y_Train_resamp[i] == Train_Predicted[i]):\n",
    "            Equal += 1\n",
    "            non_default += 1\n",
    "        \n",
    "Probs\n",
    "\n",
    "Score = Probs/(1 - Probs)\n",
    "print(np.mean(Score[Y_Train_resamp == 0]), np.mean(Score[Y_Train_resamp == 1]))\n",
    "\n",
    "\n",
    "Test_Predicted = np.zeros(len(Probs_test))\n",
    "for i in range(len(Probs_test)):\n",
    "    if (Probs_test[i] < .5):  \n",
    "        Test_Predicted[i] = 0\n",
    "    else:\n",
    "        Test_Predicted[i] = 1\n",
    "\n",
    "Equal_test = 0\n",
    "default_test = 0\n",
    "non_default_test = 0\n",
    "for i in range(len(Y_Test)):\n",
    "    if(Y_Test[i] == 1):\n",
    "        if(Y_Test[i] == Test_Predicted[i]):\n",
    "            Equal_test += 1\n",
    "            default_test += 1\n",
    "    else:\n",
    "        if(Y_Test[i] == Test_Predicted[i]):\n",
    "            Equal_test += 1\n",
    "            non_default_test += 1\n",
    "        \n",
    "Probs\n",
    "\n",
    "Score_test = Probs_test/(1 - Probs_test)\n",
    "print(np.mean(Score_test[Y_Test == 0]), np.mean(Score_test[Y_Test == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(Train_Predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Accuracy on Training Set is:\")\n",
    "print(Equal/len(Y_Train_resamp))\n",
    "print(\"Classification Accuracy on Non-Default Training Set is:\")\n",
    "print(Non_Default/(len(Y_Train_resamp) - np.sum(Y_Train_resamp)))\n",
    "print(\"Classification Accuracy on Default Training Set is:\")\n",
    "print(Default/np.sum(Y_Train_resamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Accuracy on Test Set is:\")\n",
    "print(Equal_test/len(Y_Test))\n",
    "print(\"Classification Accuracy on Non-Default Test Set is:\")\n",
    "print(non_default_test/(len(Y_Test) - np.sum(Y_Test)))\n",
    "print(\"Classification Accuracy on Default Test Set is:\")\n",
    "print(default_test/np.sum(Y_Test))"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
